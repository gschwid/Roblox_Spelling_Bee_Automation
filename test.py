from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer
import torchaudio
from torchaudio.transforms import Resample

modelPath = 'whisper-spelling-bee'

model = WhisperForConditionalGeneration.from_pretrained(modelPath)
feature_extractor = WhisperFeatureExtractor.from_pretrained(modelPath)
tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-tiny')
audio_path = 'dataset/word173.wav'
waveform, sample_rate = torchaudio.load(audio_path)
resampler = Resample(sample_rate, 16000, dtype=waveform.dtype)
resampled_waveform = resampler(waveform)
waveform = resampled_waveform.squeeze()

# Preprocess the audio input
inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors="pt", padding=True)

# Explicitly pass the attention mask
input_features = inputs["input_features"]
attention_mask = inputs.get("attention_mask")  # Generated by the processor

# Perform inference
predicted_ids = model.generate(input_features, attention_mask=attention_mask)
transcription = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)

print("Transcription:", transcription[0])

